{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rq8fcpdMrcP"
      },
      "source": [
        "### Lab 8.1 Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_P0NlqaMrcQ"
      },
      "source": [
        "In this lab you will explore different methods of text tokenization.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-njxMIgPMrcR"
      },
      "source": [
        "The following code will download the text of Shakespeare's sonnets and read it in as one long string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "olE-G3oDMrcR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5KAo0-3MrcS",
        "outputId": "45996ebd-63d9-4eab-eb7b-1a2b3f50b044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-25 04:43:44--  https://www.dropbox.com/scl/fi/7r68l64ijemidyb9lf80q/sonnets.txt?rlkey=udb47coatr2zbrk31hsfbr22y&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.4.18, 2620:100:6019:18::a27d:412\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.4.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc42f7dccf91b538449f315cda16.dl.dropboxusercontent.com/cd/0/inline/CkzF72MfElOg_kY-xFMBv_xBkcHgtbdvxdU3j32Sflfyu-ICTQPbH9yAgz1gbomAfl6Y2tIvvVOHH8gu9UoeDlaKq6goW2orDtHWbCToY9Cn-2_6KhvJfbAg0-j6iyARg1fNKfxB3w3JlzKOQS3u29gn/file?dl=1# [following]\n",
            "--2025-02-25 04:43:45--  https://uc42f7dccf91b538449f315cda16.dl.dropboxusercontent.com/cd/0/inline/CkzF72MfElOg_kY-xFMBv_xBkcHgtbdvxdU3j32Sflfyu-ICTQPbH9yAgz1gbomAfl6Y2tIvvVOHH8gu9UoeDlaKq6goW2orDtHWbCToY9Cn-2_6KhvJfbAg0-j6iyARg1fNKfxB3w3JlzKOQS3u29gn/file?dl=1\n",
            "Resolving uc42f7dccf91b538449f315cda16.dl.dropboxusercontent.com (uc42f7dccf91b538449f315cda16.dl.dropboxusercontent.com)... 162.125.4.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uc42f7dccf91b538449f315cda16.dl.dropboxusercontent.com (uc42f7dccf91b538449f315cda16.dl.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100539 (98K) [application/binary]\n",
            "Saving to: ‘sonnets.txt’\n",
            "\n",
            "sonnets.txt         100%[===================>]  98.18K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-02-25 04:43:45 (5.35 MB/s) - ‘sonnets.txt’ saved [100539/100539]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-clobber \"https://www.dropbox.com/scl/fi/7r68l64ijemidyb9lf80q/sonnets.txt?rlkey=udb47coatr2zbrk31hsfbr22y&dl=1\" -O sonnets.txt\n",
        "text = (open(\"sonnets.txt\").read())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OSDexK9UMrcT"
      },
      "outputs": [],
      "source": [
        "text = text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjxxWgEiMrcT",
        "outputId": "bec42d3f-7539-4cf0-b2ed-c6c125befce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i\n",
            "\n",
            " from fairest creatures we desire increase,\n",
            " that thereby beauty's rose might never die,\n",
            " but as the riper should by time decease,\n",
            " his tender heir might bear his memory:\n",
            " but thou, contracted to thine own bright eyes,\n",
            " feed'st thy light's flame with self-substantial fuel,\n",
            " making a famine where abundance lies,\n",
            " thy self thy foe, to thy sweet self too cruel:\n",
            " thou that art now the world's fresh ornament,\n",
            " and only herald to the gaudy spring,\n",
            " within thine own bud buriest thy content,\n",
            " and tender churl mak'st waste in niggarding:\n",
            "   pity the world, or else this glutton be,\n",
            "   to eat the world's due, by the grave and thee.\n",
            "\n",
            " ii\n",
            "\n",
            " when forty winters shall besiege thy brow,\n",
            " and dig deep trenches in thy beauty's field,\n",
            " thy youth's proud livery so gazed on now,\n",
            " will be a tatter'd weed of small worth held:\n",
            " then being asked, where all thy beauty lies,\n",
            " where all the treasure of thy lusty days;\n",
            " to say, within thine own deep sunken eyes,\n",
            " were an all-eating shame, and thriftless praise.\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlUpAiBBMrcU"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1. Prepare a vocabulary of the unique words in the dataset.  (For simplicity's sake you can leave the punctuation in.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rOemH19FNao-"
      },
      "outputs": [],
      "source": [
        "vocab_words = set(text.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sAAEmsfMrcU"
      },
      "source": [
        "2. Now you will make a Dataset subclass that can return sequences of tokens, encoded as integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "818AffTCMrcV"
      },
      "outputs": [],
      "source": [
        "class WordDataset(Dataset):\n",
        "  def __init__(self,text,seq_len=100):\n",
        "    self.seq_len = seq_len\n",
        "    self.vocab = set(text.split())\n",
        "    self.vocab_size = len(self.vocab)\n",
        "    self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
        "    self.indices = [self.word2idx[word] for word in text.split()]\n",
        "    # add code to compute the vocabulary (copied from exercise 1)\n",
        "    # add code to convert the text to a sequence of word indices\n",
        "\n",
        "  def __len__(self):\n",
        "    # replace this with code to return the number of possible sub-sequences\n",
        "    return len(self.indices) - self.seq_len\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    # replace this with code to return the sequence of token indices starting at i and the index of token i+seq_len as the label\n",
        "    return self.indices[i:i+self.seq_len], self.indices[i+self.seq_len]\n",
        "\n",
        "  def decode(self,tokens):\n",
        "    # replace this with code to convert a sequence of tokens back into a string\n",
        "    return ' '.join([list(self.vocab)[token] for token in tokens])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i5VfsOhMrcV"
      },
      "source": [
        "3. Verify that your class can successfully encode and decode sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMUqb9z4aIEN",
        "outputId": "6ab96368-73bc-4455-b32b-3d23014dc74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2435, 3715, 1749, 1450, 2913, 1870, 2518, 972, 2352, 3517, 1169, 3444, 764, 4479, 1803, 408, 733, 2260, 2274, 956, 3908, 478, 71, 4007, 3843, 3444, 1393, 71, 3634, 1803, 2463, 3569, 375, 3665, 3982, 4581, 3531, 557, 1469, 2824, 4182, 1370, 1901, 3838, 2709, 859, 4503, 745, 4273, 4238, 1469, 4528, 1469, 760, 375, 1469, 3465, 4528, 1162, 792, 1821, 972, 820, 4013, 733, 3000, 4027, 1984, 3382, 1426, 1584, 375, 733, 4571, 3484, 790, 3665, 3982, 4186, 2768, 1469, 4537, 3382, 4007, 3844, 3888, 4329, 1678, 101, 91, 733, 1166, 1569, 3212, 24, 3722, 3145, 375, 2046, 733]\n",
            "﻿i from fairest creatures we desire increase, that thereby beauty's rose might never die, but as the riper should by time decease, his tender heir might bear his memory: but thou, contracted to thine own bright eyes, feed'st thy light's flame with self-substantial fuel, making a famine where abundance lies, thy self thy foe, to thy sweet self too cruel: thou that art now the world's fresh ornament, and only herald to the gaudy spring, within thine own bud buriest thy content, and tender churl mak'st waste in niggarding: pity the world, or else this glutton be, to eat the\n"
          ]
        }
      ],
      "source": [
        "dataset_word = WordDataset(text)\n",
        "coded_sequence, label = dataset_word.__getitem__(0)\n",
        "print(coded_sequence)\n",
        "print(dataset_word.decode(coded_sequence))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByH4zlBMrcV"
      },
      "source": [
        "4. Do the exercise again, but this time at the character level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mXz15uSQMrcV"
      },
      "outputs": [],
      "source": [
        "class CharacterDataset(Dataset):\n",
        "  def __init__(self,text,seq_len=100):\n",
        "    self.seq_len = seq_len\n",
        "    self.vocab = set(text)\n",
        "    self.vocab_size = len(self.vocab)\n",
        "    self.char2idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
        "    self.indices = [self.char2idx[char] for char in text]\n",
        "    # add code to compute the vocabulary of unique characters\n",
        "    # add code to convert the text to a sequence of character indices\n",
        "\n",
        "  def __len__(self):\n",
        "    # replace this with code to return the number of possible sub-sequences\n",
        "    return len(self.indices) - self.seq_len\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    # replace this with code to return the sequence of token indices starting at i and the index of token i+seq_len as the label\n",
        "    return self.indices[i:i+self.seq_len], self.indices[i+self.seq_len]\n",
        "\n",
        "  def decode(self,tokens):\n",
        "    # replace this with code to convert a sequence of tokens back into a string\n",
        "    return ''.join([list(self.vocab)[token] for token in tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1zTzCB1b_Zq",
        "outputId": "23f14826-0fec-4a64-e9e5-0a85186187a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9, 13, 21, 21, 26, 3, 17, 24, 19, 26, 3, 10, 13, 17, 15, 5, 27, 26, 29, 17, 15, 10, 27, 23, 17, 15, 5, 26, 1, 15, 26, 16, 15, 5, 13, 17, 15, 26, 13, 0, 29, 17, 15, 10, 5, 15, 28, 21, 26, 27, 8, 10, 27, 26, 27, 8, 15, 17, 15, 20, 22, 26, 20, 15, 10, 23, 27, 22, 31, 5, 26, 17, 24, 5, 15, 26, 19, 13, 12, 8, 27, 26, 0, 15, 25, 15, 17, 26, 16, 13, 15, 28, 21, 26, 20, 23, 27, 26, 10, 5]\n",
            "﻿i\n",
            "\n",
            " from fairest creatures we desire increase,\n",
            " that thereby beauty's rose might never die,\n",
            " but as\n"
          ]
        }
      ],
      "source": [
        "dataset_char = CharacterDataset(text)\n",
        "coded_sequence, label = dataset_char.__getitem__(0)\n",
        "print(coded_sequence)\n",
        "print(dataset_char.decode(coded_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EelZ7ruQMrcV"
      },
      "source": [
        "5. Compare the number of sequences for each tokenization method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlBXR3jXcG2C",
        "outputId": "6febdd27-c9c7-48ec-8796-ad3068e8f0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sequences for word tokenization: 17570\n",
            "Number of sequences for character tokenization: 97820\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of sequences for word tokenization: {dataset_word.__len__()}')\n",
        "print(f'Number of sequences for character tokenization: {dataset_char.__len__()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpUfRxZOMrcV"
      },
      "source": [
        "6. Optional: implement the byte pair encoding algorithm to make a Dataset class that uses word parts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary of Lab 8\n",
        "\n",
        "Lab 8 focused on exploring different methods of text tokenization using Shakespeare's sonnets. The lab included exercises on creating vocabulary from the text, implementing Dataset subclasses for word and character tokenization, encoding and decoding sequences, and comparing the number of sequences generated by each method. Optional exercises included implementing byte pair encoding."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
